{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0fd33c78",
      "metadata": {
        "id": "0fd33c78"
      },
      "source": [
        "# Valuation of Housing Developments in SF"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "jgKbdfkuVIai"
      },
      "id": "jgKbdfkuVIai"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2865c69",
      "metadata": {
        "id": "e2865c69"
      },
      "outputs": [],
      "source": [
        "# Importing Necessary Packages\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "import folium\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import reverse_geocoder as rg\n",
        "import pprint\n",
        "import geocoder\n",
        "import multiprocessing as mp\n",
        "import geopy\n",
        "from geopy.geocoders import Nominatim\n",
        "from geopy.point import Point\n",
        "import geopandas as gpd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import GradientBoostingRegressor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49665180",
      "metadata": {
        "id": "49665180"
      },
      "source": [
        "## Table of Contents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aed112e0",
      "metadata": {
        "id": "aed112e0"
      },
      "source": [
        "### 1. Exploratory Data Analysis and Cleaning\n",
        "####    a. Valuations Dataset\n",
        "####    b. Police Dataset\n",
        "####    c. City Survey Dataset\n",
        "####    d. Final Dataset Join\n",
        "### 2. Baseline Model (Linear Regression)\n",
        "####    a. Prep/Split/Preprocessing\n",
        "####    b. Model Definition/Run Model\n",
        "####    c. Model Results\n",
        "### 3. Final Model (Random Forest)\n",
        "####    a. Prep/Split/Preprocessing\n",
        "####    b. Model Definition/Run Model\n",
        "####    c. Model Results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17cdb520",
      "metadata": {
        "id": "17cdb520"
      },
      "source": [
        "## 1. Exploratory Data Analysis and Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4325743c",
      "metadata": {
        "id": "4325743c"
      },
      "source": [
        "### 1a. Valuations Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in valuation data\n",
        "valuation = pd.read_csv(\"Assessor_Historical_Secured_Property_Tax_Rolls.csv\", low_memory=False)"
      ],
      "metadata": {
        "id": "m_iRZu-O6rbU"
      },
      "id": "m_iRZu-O6rbU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop nan values in geom data\n",
        "val2 = valuation.dropna(subset = ['the_geom'])\n",
        "val2.the_geom"
      ],
      "metadata": {
        "id": "5czMBkZ68LCj"
      },
      "id": "5czMBkZ68LCj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Format geom data for our use\n",
        "val2[\"x\"] = val2[\"the_geom\"].str.split(\"\\s+\").str[1].str.replace(\"\\(\", \"\")\n",
        "val2[\"y\"] = val2[\"the_geom\"].str.split(\"\\s+\").str[2].str.rstrip(\")\")\n",
        "\n",
        "# Convert extracted values to numeric type\n",
        "val2[\"x\"] = pd.to_numeric(val2[\"x\"])\n",
        "val2[\"y\"] = pd.to_numeric(val2[\"y\"])\n",
        "val2[\"x\"] "
      ],
      "metadata": {
        "id": "siL5uq5P8YhU"
      },
      "id": "siL5uq5P8YhU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in Zip GeoDF\n",
        "data_poly = gpd.read_file(\"San Francisco ZIP Codes.geojson\")"
      ],
      "metadata": {
        "id": "_ce5F_og8l6a"
      },
      "id": "_ce5F_og8l6a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert val2 to GeoDF and join with Zip GeoDF\n",
        "\n",
        "gdf = gpd.GeoDataFrame(val2, geometry=gpd.points_from_xy(val2.x, val2.y))\n",
        "joined_gdf = gpd.sjoin(gdf, data_poly, op='within')"
      ],
      "metadata": {
        "id": "MftgvXRH8uWv"
      },
      "id": "MftgvXRH8uWv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display Columns in Joined DF\n",
        "joined_gdf.columns"
      ],
      "metadata": {
        "id": "1v9-77AG9vBJ"
      },
      "id": "1v9-77AG9vBJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose Relevant Columns\n",
        "val_clean = joined_gdf[[\"Closed Roll Year\", \"id\", \"Assessed Improvement Value\", \"Assessed Land Value\", \"Assessed Personal Property Value\",\n",
        "\"Year Property Built\", \"Number of Bathrooms\",\"Number of Bedrooms\", \"Number of Rooms\", \"Number of Stories\", \"Number of Units\", \"Assessor Neighborhood District\", \"Assessor Neighborhood Code\"]]"
      ],
      "metadata": {
        "id": "VDNGvkbY9zVt"
      },
      "id": "VDNGvkbY9zVt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep only data from the years 2013 through 2019\n",
        "val_clean = val_clean[(val_clean[\"Closed Roll Year\"] >= 2013) & (val_clean[\"Closed Roll Year\"] <= 2019)]\n",
        "\n",
        "# Create Total Assesed Value which equals Assessed Personal Property Value + Assessed Improvement Value\n",
        "val_clean[\"Total Assessed Value\"] = val_clean[\"Assessed Land Value\"] + val_clean[\"Assessed Improvement Value\"]\n",
        "val_clean = val_clean.rename(columns = {\"id\":\"Zipcode\"})"
      ],
      "metadata": {
        "id": "YMlTiCEX94zT"
      },
      "id": "YMlTiCEX94zT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for invalid zipcodes (none)\n",
        "val_clean[\"Zipcode\"].value_counts()"
      ],
      "metadata": {
        "id": "fua4rTSq-EJS"
      },
      "id": "fua4rTSq-EJS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check to look at Total Assessed Value\n",
        "val_clean[\"Total Assessed Value\"].value_counts()"
      ],
      "metadata": {
        "id": "XeezD99c-D91"
      },
      "id": "XeezD99c-D91",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the rows where Total Assessed Value = 0\n",
        "val_clean = val_clean[val_clean[\"Total Assessed Value\"] != 0]\n",
        "val_clean[\"Total Assessed Value\"].value_counts()"
      ],
      "metadata": {
        "id": "gG053xBR-_hB"
      },
      "id": "gG053xBR-_hB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a precursory look at val_clean\n",
        "val_clean.describe()"
      ],
      "metadata": {
        "id": "cJbh_IF1BXJq"
      },
      "id": "cJbh_IF1BXJq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at Distribution of Total Assessed Values\n",
        "plt.hist(val_clean['Total Assessed Value'], bins=50)\n",
        "plt.xlabel('Total Assessed Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ORpVex1vBpOy"
      },
      "id": "ORpVex1vBpOy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at how year built may affect Total Assessed Value\n",
        "sns.scatterplot(x='Year Property Built', y='Total Assessed Value', data=val_clean)\n",
        "plt.xlabel('Year Property Built')\n",
        "plt.ylabel('Total Assessed Value')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7mlNe2rHBxZ8"
      },
      "id": "7mlNe2rHBxZ8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at how many properties are in each assessment district\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.countplot(x='Assessor Neighborhood District', data=val_clean)\n",
        "plt.xlabel('Assessor Neighborhood District')\n",
        "plt.ylabel('Number of Properties')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rCFyGbyTCGX5"
      },
      "id": "rCFyGbyTCGX5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping property values by zip code\n",
        "map_sf = folium.Map(location=[37.7749, -122.4194], zoom_start=12)\n",
        "choropleth = folium.Choropleth(geo_data='San Francisco ZIP Codes.geojson', name='choropleth', data=val_clean, columns=['Zipcode', 'Total Assessed Value'], key_on='feature.properties.zipcode', fill_color='YlOrRd', fill_opacity=0.7, line_opacity=0.2, legend_name='Total Assessed Value').add_to(map_sf)\n",
        "folium.LayerControl().add_to(map_sf)\n",
        "map_sf"
      ],
      "metadata": {
        "id": "r2AbLOjKCSd2"
      },
      "id": "r2AbLOjKCSd2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "61b482f6",
      "metadata": {
        "id": "61b482f6"
      },
      "source": [
        "### 1b. Police Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in SFPD Crime Datasets\n",
        "\n",
        "police_df1318 = pd.read_csv(\"PoliceDepartment_IncidentReports_2003_May2018.csv\")\n",
        "police_df_18 = pd.read_csv(\"Police_Department_Incident_Reports__2018_to_Present.csv\")"
      ],
      "metadata": {
        "id": "OCag4cWWDsrz"
      },
      "id": "OCag4cWWDsrz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create geometry columns in order to extract zipcode later\n",
        "police_df_18['geometry'] = police_df_18['Point']\n",
        "police_df1318['geometry'] = police_df1318['location']"
      ],
      "metadata": {
        "id": "SfLsWtOMEEmw"
      },
      "id": "SfLsWtOMEEmw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop unneeded columns\n",
        "police_df13 = police_df1318.drop(columns=['DELETE - 2017 Fix It Zones 2 2', 'Civic Center Harm Reduction Project Boundary 2 2',\n",
        "          'Fix It Zones as of 2017-11-06  2 2', 'DELETE - HSOC Zones 2 2', \"location\",\n",
        "          'Fix It Zones as of 2018-02-07 2 2', 'CBD, BID and GBD Boundaries as of 2017 2 2', \n",
        "          'Central Market/Tenderloin Boundary 2 2', 'Central Market/Tenderloin Boundary Polygon - Updated 2 2',\n",
        "          'HSOC Zones as of 2018-06-05 2 2', 'OWED Public Spaces 2 2', 'DELETE - Fire Prevention Districts 2 2','DELETE - Police Districts 2 2',\n",
        "          'DELETE - Supervisor Districts 2 2','DELETE - Zip Codes 2 2','DELETE - Neighborhoods 2 2',\n",
        "          'SF Find Neighborhoods 2 2', 'Current Police Districts 2 2','Time',\n",
        "          'Current Supervisor Districts 2 2','Analysis Neighborhoods 2 2', 'DayOfWeek', 'Resolution',\n",
        "          'Areas of Vulnerability, 2016 2 2','Neighborhoods 2', 'Incident Code', 'Descript'])\n",
        "\n",
        "police_df18 = police_df_18.drop(columns=['Report Datetime', 'Row ID','Incident Subcategory', 'Incident Description',\n",
        "        'Resolution', 'Intersection', 'CNN', \n",
        "        'Analysis Neighborhood', 'Supervisor District','Point',\n",
        "        'Supervisor District 2012', 'CAD Number',\n",
        "        'Report Type Code', 'Report Type Description', 'Filed Online',\n",
        "        'Incident Code',\n",
        "        'Neighborhoods', 'ESNCAG - Boundary File',\n",
        "        'Central Market/Tenderloin Boundary Polygon - Updated','Incident Day of Week', \n",
        "        'Civic Center Harm Reduction Project Boundary', 'Incident Date', 'Incident Time', 'Incident Year',\n",
        "        'HSOC Zones as of 2018-06-05', 'Invest In Neighborhoods (IIN) Areas',\n",
        "        'Current Supervisor Districts', 'Current Police Districts'])"
      ],
      "metadata": {
        "id": "1zp--mjxEN5I"
      },
      "id": "1zp--mjxEN5I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Format 2018-Present to prepare for combining the two separate sets.\n",
        "police_df18 = police_df18.rename(columns = {'Incident ID':'PdId', 'Incident Number': 'IncidntNum', 'Longitude':'X', 'Latitude':'Y',\n",
        "                                           'Incident Datetime':'Date', 'Police District':'PdDistrict', 'Incident Category': 'Category',\n",
        "                                           'geometry':'geometry'})\n",
        "\n",
        "# Placeholder for address for 2018-present data\n",
        "police_df18['Address'] = 0"
      ],
      "metadata": {
        "id": "UB7JoebZEqjA"
      },
      "id": "UB7JoebZEqjA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Double-check whether columns are the same across 2013-2018 & 2018-Present dataset\n",
        "police_df13.columns, police_df18.columns #True"
      ],
      "metadata": {
        "id": "zdEnbBsLE96R"
      },
      "id": "zdEnbBsLE96R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Append dataframes\n",
        "police_df18 = police_df18[['PdId', 'IncidntNum', 'Category', 'Date', 'PdDistrict', 'Address','X', 'Y', 'geometry']]\n",
        "police_df13.columns == police_df18.columns\n",
        "police = pd.concat([police_df18, police_df13])"
      ],
      "metadata": {
        "id": "17u7n6VQFFko"
      },
      "id": "17u7n6VQFFko",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out overlap for 2018 & any other duplicates\n",
        "police = police.drop_duplicates()\n",
        "police.duplicated().unique()"
      ],
      "metadata": {
        "id": "U3VviKruFITA"
      },
      "id": "U3VviKruFITA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Date column to datetime\n",
        "police['Incident Date'] = pd.to_datetime(police['Date'])\n",
        "police['Year'] = police['Incident Date'].dt.year # create a new Year column\n",
        "police"
      ],
      "metadata": {
        "id": "ZgbyAWOdFMuW"
      },
      "id": "ZgbyAWOdFMuW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter for years 2013-2019\n",
        "police_filtered = police.loc[police['Year'] <= 2019]\n",
        "police_filtered = police_filtered.loc[police_filtered['Year'] >= 2013]\n",
        "police_filtered"
      ],
      "metadata": {
        "id": "xn7YcLlfFSw9"
      },
      "id": "xn7YcLlfFSw9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at number of incidents per year\n",
        "\n",
        "sns.set_style('darkgrid')\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='Year', data=police_filtered)\n",
        "plt.title('Incidents by Year')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I3D9FxvmFZg5"
      },
      "id": "I3D9FxvmFZg5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at number of incidents by police district\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.countplot(y='PdDistrict', data=police_filtered, order=police_filtered['PdDistrict'].value_counts().index)\n",
        "plt.title('Incidents by Police District')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "k0MSEl7IFjZU"
      },
      "id": "k0MSEl7IFjZU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's map the incidents\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "scatterplot = sns.scatterplot(x='X', y='Y', hue='Category', data=police_filtered, alpha=0.5, legend='brief')\n",
        "scatterplot.legend(loc='center right', bbox_to_anchor=(1.3, 0.5), ncol=1)\n",
        "plt.title('Incident Category by Location')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TfcfUoWXFvXd"
      },
      "id": "TfcfUoWXFvXd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop all missing coordinate values\n",
        "police_filtered = police_filtered.dropna(subset=['X','Y'], how='all')\n",
        "police_filtered.head()"
      ],
      "metadata": {
        "id": "WR1idywmFzjc"
      },
      "id": "WR1idywmFzjc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert latitude and longitude to zipcode\n",
        "data_poly = gpd.read_file(\"San Francisco ZIP Codes.geojson\")"
      ],
      "metadata": {
        "id": "jTu3k6n8F21j"
      },
      "id": "jTu3k6n8F21j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Police GeoDF and join with Zip Code GDF\n",
        "gdf = gpd.GeoDataFrame(police_filtered, geometry=gpd.points_from_xy(police_filtered.X, police_filtered.Y))\n",
        "joined_gdf = gpd.sjoin(gdf, data_poly, op='within')"
      ],
      "metadata": {
        "id": "WJm3bpBSF6gK"
      },
      "id": "WJm3bpBSF6gK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the count of instances per zip code\n",
        "joined_gdf['id'].value_counts()"
      ],
      "metadata": {
        "id": "l80qcndeHo9R"
      },
      "id": "l80qcndeHo9R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose relevant columns and get info\n",
        "police_final = joined_gdf[['PdId', 'IncidntNum', 'Category', 'Date', 'Year', 'id']].rename(columns = {'id': 'zipcode'})\n",
        "police_final.info()"
      ],
      "metadata": {
        "id": "EYfjPNSdHvVQ"
      },
      "id": "EYfjPNSdHvVQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d8a3d3c4",
      "metadata": {
        "id": "d8a3d3c4"
      },
      "source": [
        "### 1c. City Survey Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in City Survey data\n",
        "xls = pd.ExcelFile('City Survey Master Data 1996-2019.xlsx')\n",
        "city_df = pd.read_excel(xls, 'Historical Data')"
      ],
      "metadata": {
        "id": "WsJ1Mb4JILS5"
      },
      "id": "WsJ1Mb4JILS5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Isolate columns of interest\n",
        "columns = [ \"year\", \"dem_zip\", \"dem_district\", \"inf_clean\", \"inf_stcond\", \"inf_sidecond\", \"inf_clean_side\", \"inf_clean_st\", \"safe_day\", \"safe_night\", \"dem_hhsize\"]\n",
        "dfagg = city_df[columns]"
      ],
      "metadata": {
        "id": "DpKna2zCIpg2"
      },
      "id": "DpKna2zCIpg2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Isolate columns for cleanliness & exclude missing values/columns with ratings of 6 or 7\n",
        "relevant_columns = [\"inf_clean\", \"inf_stcond\", \"inf_sidecond\", \"inf_clean_side\", \"inf_clean_st\"]\n",
        "rating_map = {\n",
        "    1: 1,\n",
        "    2: 2,\n",
        "    3: 3,\n",
        "    4: 4,\n",
        "    5: 5,\n",
        "}\n",
        "dfagg[relevant_columns] = dfagg[relevant_columns].replace(rating_map)"
      ],
      "metadata": {
        "id": "3FSkEYQFIszz"
      },
      "id": "3FSkEYQFIszz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the ratings to numeric values and replace non-integer values with NaN\n",
        "dfagg[relevant_columns] = dfagg[relevant_columns].applymap(lambda x: pd.to_numeric(x, errors='coerce'))"
      ],
      "metadata": {
        "id": "bgL5gnoAI3e_"
      },
      "id": "bgL5gnoAI3e_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the mean for each row, excluding NaN values\n",
        "dfagg['cleanliness'] = dfagg[relevant_columns].mean(axis=1, skipna=True)"
      ],
      "metadata": {
        "id": "JZuMELckI8It"
      },
      "id": "JZuMELckI8It",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the result\n",
        "dfagg['cleanliness'].value_counts()"
      ],
      "metadata": {
        "id": "nkrFcGNfJB17"
      },
      "id": "nkrFcGNfJB17",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a cleaned df for 2019\n",
        "dfagg = dfagg.drop(relevant_columns, axis=1)\n",
        "df2019_cleaned = dfagg.rename(columns={\"dem_zip\":\"zipcode\", \"dem_district\": \"district\",\"dem_hhsize\":\"household_size\"})\n",
        "df2019_cleaned"
      ],
      "metadata": {
        "id": "0XZ4WuwBJlnh"
      },
      "id": "0XZ4WuwBJlnh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find entries without/invalid zipcodes & replace with 0\n",
        "df2019_cleaned['zipcode'] = df2019_cleaned['zipcode'].fillna(0).astype(int)\n",
        "df2019_cleaned['zipcode'] = df2019_cleaned['zipcode'].replace([88888, 99999], 0)"
      ],
      "metadata": {
        "id": "GLShCoMeJ-Zg"
      },
      "id": "GLShCoMeJ-Zg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter for years 2013-2019\n",
        "survey_filtered = df2019_cleaned.loc[df2019_cleaned['year'] <= 2019]\n",
        "survey_filtered = survey_filtered.loc[df2019_cleaned['year'] >= 2013]\n",
        "survey_filtered\n",
        "\n",
        "# Notes: No zipcodes available for 2015, Only have data every other year starting 2005"
      ],
      "metadata": {
        "id": "RjIfLMYQKH33"
      },
      "id": "RjIfLMYQKH33",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the data by year and zipcode and count the occurrences\n",
        "grouped = survey_filtered.groupby(['year', 'zipcode']).size().unstack()"
      ],
      "metadata": {
        "id": "g4RhYga2LDr0"
      },
      "id": "g4RhYga2LDr0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot occurences by year and zipcode\n",
        "ax = grouped.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
        "\n",
        "ax.set_xlabel('Year') \n",
        "ax.set_ylabel('Count')\n",
        "ax.legend(title='Zipcode', loc='upper right')\n",
        "ax.set_title('Distribution of Zipcodes by Year')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5YbiF8pmL1As"
      },
      "id": "5YbiF8pmL1As",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at count of responses by year\n",
        "sns.set_style('darkgrid')\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='year', data=survey_filtered)\n",
        "plt.title('Number of Responses by Year')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FrA7oogvM-LX"
      },
      "id": "FrA7oogvM-LX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2015 has no zipcode data\n",
        "survey_filtered[survey_filtered['year'] == 2015]"
      ],
      "metadata": {
        "id": "gvJ8z7PmNLsk"
      },
      "id": "gvJ8z7PmNLsk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unable to identify correct zipcode for zipcode value 0 (missing data) by district since there are multiiple zipcodes per district\n",
        "survey_filtered.groupby(['district', 'zipcode']).count()"
      ],
      "metadata": {
        "id": "HWZKGroVNSzf"
      },
      "id": "HWZKGroVNSzf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill in gap years with previous year data (i.e. fill in 2014 & 2015 & 2016 data with 2013 data)\n",
        "\n",
        "# Drop 2015 data without zipcodes\n",
        "survey_filtered1 = survey_filtered[survey_filtered.zipcode != 0]\n",
        "survey_filtered1.year.value_counts()\n",
        "\n",
        "# Want 2013-2019\n",
        "\n",
        "# Create 2014, 2015, 2016 data with 2013 data\n",
        "df2014 = survey_filtered1[survey_filtered1.year == 2013]\n",
        "df2014['year'] = df2014['year'].replace(2013, 2014)\n",
        "df2015 = survey_filtered1[survey_filtered1.year == 2013]\n",
        "df2015['year'] = df2015['year'].replace(2013, 2015)\n",
        "df2016 = survey_filtered1[survey_filtered1.year == 2013]\n",
        "df2016['year'] = df2016['year'].replace(2013, 2016)\n",
        "\n",
        "# Create 2018 data with 2017 data\n",
        "df2018 = survey_filtered1[survey_filtered1.year == 2017]\n",
        "df2018['year'] = df2018['year'].replace(2017, 2018)\n",
        "\n",
        "# Append all dataframes back to original df --> survey_filtered\n",
        "survey_final = pd.concat([survey_filtered1, df2014, df2015, df2016, df2018])\n",
        "survey_final"
      ],
      "metadata": {
        "id": "M-EHbE7GNuI4"
      },
      "id": "M-EHbE7GNuI4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the data by zip and day\n",
        "grouped = survey_final.groupby(['zipcode', 'safe_day']).size().unstack()\n",
        "\n",
        "# Plot a stacked bar chart\n",
        "ax = grouped.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
        "ax.set_xlabel('Zipcode') \n",
        "ax.set_ylabel('Count')\n",
        "ax.legend(title='Safety Rating (5: Highest, 1: Lowest)', loc='upper right')\n",
        "ax.set_title('Distribution of Safety During Day by Zipcode')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9kl5SwJhN49y"
      },
      "id": "9kl5SwJhN49y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the data by zip and night\n",
        "grouped = survey_final.groupby(['zipcode', 'safe_night']).size().unstack()\n",
        "\n",
        "# Plot a stacked bar chart\n",
        "ax = grouped.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
        "ax.set_xlabel('Zipcode') \n",
        "ax.set_ylabel('Count')\n",
        "ax.legend(title='Safety Rating (5: Highest, 1: Lowest)', loc='upper right')\n",
        "ax.set_title('Distribution of Safety During Night by Zipcode')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g5EHNWtxOB4K"
      },
      "id": "g5EHNWtxOB4K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1d. Final Dataset Join"
      ],
      "metadata": {
        "id": "6-FxxcO9QcPS"
      },
      "id": "6-FxxcO9QcPS"
    },
    {
      "cell_type": "code",
      "source": [
        "# Final dataframes for Years 2013-2019 (will join with valuation data)\n",
        "display(survey_final)\n",
        "display(police_final)"
      ],
      "metadata": {
        "id": "GHqCs-p-QkV_"
      },
      "id": "GHqCs-p-QkV_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Format valuation data in order to join\n",
        "val_final = val_clean.rename(columns = {'Closed Roll Year': 'Year'})\n",
        "val_final.head()"
      ],
      "metadata": {
        "id": "bwdGYFjpRFv3"
      },
      "id": "bwdGYFjpRFv3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the count of incidents from police_final \n",
        "police_final2 = police_final.groupby(['Year','zipcode']).count().reset_index().rename(columns = {'PdId':'Incident Counts', 'zipcode':'Zipcode'})\n",
        "police_final2 = police_final2.drop(columns = ['IncidntNum','Date', 'Category'])\n",
        "police_final2"
      ],
      "metadata": {
        "id": "fgHW4mLnRtbN"
      },
      "id": "fgHW4mLnRtbN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get average of safe_day,safe_night,household_size,cleanliness for each year,zipcode pair\n",
        "survey_final2 = survey_final.groupby(['year','zipcode']).mean().reset_index().drop(columns = ['district']).rename(columns = {'year': 'Year', 'zipcode':'Zipcode'})\n",
        "survey_final2"
      ],
      "metadata": {
        "id": "XclmuSO3Rv7t"
      },
      "id": "XclmuSO3Rv7t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check datatypes\n",
        "#val_final.info(), police_final2.info(), survey_final2.info()\n",
        "\n",
        "# Convert police_final2 Zipcode column from str-->int\n",
        "police_final2['Zipcode'] = police_final2.Zipcode.astype(int)\n",
        "police_final2\n",
        "\n",
        "val_final.info(), police_final2.info(), survey_final2.info()"
      ],
      "metadata": {
        "id": "LVBgqiwmR6L9"
      },
      "id": "LVBgqiwmR6L9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Left join on val_final with police_final2\n",
        "join1 = pd.merge(val_final, police_final2, on = ['Year', 'Zipcode'], how = 'left')\n",
        "join1"
      ],
      "metadata": {
        "id": "hGyNtrnzSGpY"
      },
      "id": "hGyNtrnzSGpY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop nan values\n",
        "join1 = join1.dropna(subset = ['Year Property Built'])\n",
        "join1"
      ],
      "metadata": {
        "id": "ZxoL7QWoSJwH"
      },
      "id": "ZxoL7QWoSJwH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Left join join1 with survey_final2\n",
        "final_df = pd.merge(join1, survey_final2, on = ['Year', 'Zipcode'], how = 'inner')"
      ],
      "metadata": {
        "id": "pYwSjJbhSOcN"
      },
      "id": "pYwSjJbhSOcN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation of Total Assessed Value with other features\n",
        "final_df.corr().loc['Total Assessed Value']\n",
        "\n",
        "# Drop Assessed Improvement Value & Assessed Land Value since both were used to calculate to Total Assessed Value\n",
        "final_df = final_df.drop(columns = ['Assessed Improvement Value', 'Assessed Land Value'])\n",
        "\n",
        "# Final Dataframe\n",
        "final_df"
      ],
      "metadata": {
        "id": "d2aPrsRsSTK3"
      },
      "id": "d2aPrsRsSTK3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One hot encode zipcode\n",
        "df_encoded = pd.get_dummies(final_df, columns = ['Zipcode'], drop_first = True) #avoid multicollinearity\n",
        "df_encoded.columns"
      ],
      "metadata": {
        "id": "kt2tVWutSm_f"
      },
      "id": "kt2tVWutSm_f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a20022ca",
      "metadata": {
        "id": "a20022ca"
      },
      "source": [
        "## 2. Baseline Model (Linear Regression)\n",
        "\n",
        "*Note: Model might underestimate since increase in tax rate does not keep up with real-world inflation in California.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f627438",
      "metadata": {
        "id": "8f627438"
      },
      "source": [
        "### 2a. Prep/Split/Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for nan values\n",
        "df_encoded.isna().sum()\n",
        "\n",
        "df_encoded = df_encoded.dropna()\n",
        "df_encoded.columns\n",
        "\n",
        "\n",
        "# Log of Total Assessed Value\n",
        "df_encoded['Log of Total Assessed Value'] = np.log(df_encoded['Total Assessed Value'])"
      ],
      "metadata": {
        "id": "62MDwv1aTMkm"
      },
      "id": "62MDwv1aTMkm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_test split\n",
        "X = df_encoded[['Year', 'Assessed Personal Property Value', 'Year Property Built',\n",
        "       'Number of Bathrooms', 'Number of Bedrooms', 'Number of Rooms',\n",
        "       'Number of Stories', 'Number of Units', \n",
        "       'Incident Counts', 'safe_day', 'safe_night', 'household_size',\n",
        "       'cleanliness', 'Zipcode_94103', 'Zipcode_94104', 'Zipcode_94105',\n",
        "       'Zipcode_94107', 'Zipcode_94108', 'Zipcode_94109', 'Zipcode_94110',\n",
        "       'Zipcode_94111', 'Zipcode_94112', 'Zipcode_94114', 'Zipcode_94115',\n",
        "       'Zipcode_94116', 'Zipcode_94117', 'Zipcode_94118', 'Zipcode_94121',\n",
        "       'Zipcode_94122', 'Zipcode_94123', 'Zipcode_94124', 'Zipcode_94127',\n",
        "       'Zipcode_94131', 'Zipcode_94132', 'Zipcode_94133', 'Zipcode_94134',\n",
        "       'Zipcode_94158']]\n",
        "Y = df_encoded['Log of Total Assessed Value']\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = .3, random_state = 42)"
      ],
      "metadata": {
        "id": "_sceYGz4TVxX"
      },
      "id": "_sceYGz4TVxX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cd3b230d",
      "metadata": {
        "id": "cd3b230d"
      },
      "source": [
        "### 2b. Model Definition/Run Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model definition\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, Y_train)"
      ],
      "metadata": {
        "id": "I_-cozw4TZkm"
      },
      "id": "I_-cozw4TZkm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct dataframe of each feature & the corresponding coefficients\n",
        "feature = ['Year', 'Assessed Personal Property Value', 'Year Property Built',\n",
        "       'Number of Bathrooms', 'Number of Bedrooms', 'Number of Rooms',\n",
        "       'Number of Stories', 'Number of Units', \n",
        "       'Incident Counts', 'safe_day', 'safe_night', 'household_size',\n",
        "       'cleanliness', 'Zipcode_94103', 'Zipcode_94104', 'Zipcode_94105',\n",
        "       'Zipcode_94107', 'Zipcode_94108', 'Zipcode_94109', 'Zipcode_94110',\n",
        "       'Zipcode_94111', 'Zipcode_94112', 'Zipcode_94114', 'Zipcode_94115',\n",
        "       'Zipcode_94116', 'Zipcode_94117', 'Zipcode_94118', 'Zipcode_94121',\n",
        "       'Zipcode_94122', 'Zipcode_94123', 'Zipcode_94124', 'Zipcode_94127',\n",
        "       'Zipcode_94131', 'Zipcode_94132', 'Zipcode_94133', 'Zipcode_94134',\n",
        "       'Zipcode_94158']\n",
        "\n",
        "pd.DataFrame(zip(feature, model.coef_), columns = ['feature', 'coefficients'])\n",
        "\n",
        "# Note: Zipcode_94104 & few others have many commercial buildings / tech companies"
      ],
      "metadata": {
        "id": "t_nCGi87ThUe"
      },
      "id": "t_nCGi87ThUe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred_test = model.predict(X_test)\n",
        "Y_pred_train = model.predict(X_train)\n",
        "mean_squared_error(np.exp(Y_test), np.exp(Y_pred_test))**(1/2), mean_squared_error(np.exp(Y_train), np.exp(Y_pred_train))**(1/2)"
      ],
      "metadata": {
        "id": "cN327VAPTvbN"
      },
      "id": "cN327VAPTvbN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameter Tuning for Linear Regression**"
      ],
      "metadata": {
        "id": "Y_YQlJr6T4KS"
      },
      "id": "Y_YQlJr6T4KS"
    },
    {
      "cell_type": "code",
      "source": [
        "# Narrow down features to the ones most correlated with Log of Total Assessed Value\n",
        "df_encoded.corr().loc['Total Assessed Value'].sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "GrLqGsL3UCA2"
      },
      "id": "GrLqGsL3UCA2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take all features with >0.05 or <-0.05 coefficient & redo train-test split\n",
        "\n",
        "# train_test split\n",
        "X = df_encoded[['Number of Bathrooms', 'Number of Rooms', 'Zipcode_94123','Number of Units','Zipcode_94105','Zipcode_94124','household_size'                    \n",
        ",'Zipcode_94112','Zipcode_94134','Zipcode_94122','Zipcode_94116','cleanliness','Year','Year Property Built','Zipcode_94104'                       \n",
        ",'Zipcode_94115','Number of Bedrooms','Zipcode_94109','Zipcode_94111','Number of Stories','Zipcode_94114','Zipcode_94103'                     \n",
        ",'Zipcode_94108','safe_day']]\n",
        "Y = df_encoded['Log of Total Assessed Value']\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = .3, random_state = 42)"
      ],
      "metadata": {
        "id": "pqL8asdCUH2i"
      },
      "id": "pqL8asdCUH2i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Definition\n",
        "model2 = LinearRegression()\n",
        "model2.fit(X_train, Y_train)"
      ],
      "metadata": {
        "id": "gel21yyKUOvH"
      },
      "id": "gel21yyKUOvH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature2 = ['Number of Bathrooms', 'Number of Rooms', 'Zipcode_94123','Number of Units','Zipcode_94105','Zipcode_94124','household_size'                    \n",
        ",'Zipcode_94112','Zipcode_94134','Zipcode_94122','Zipcode_94116','cleanliness','Year','Year Property Built','Zipcode_94104'                       \n",
        ",'Zipcode_94115','Number of Bedrooms','Zipcode_94109','Zipcode_94111','Number of Stories','Zipcode_94114','Zipcode_94103'                     \n",
        ",'Zipcode_94108','safe_day']\n",
        "\n",
        "# Construct dataframe of each feature & the corresponding coefficients\n",
        "pd.DataFrame(zip(feature, model2.coef_), columns = ['feature', 'coefficients'])"
      ],
      "metadata": {
        "id": "gEZGgdtLURnT"
      },
      "id": "gEZGgdtLURnT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on train set\n",
        "Y_pred_train = model2.predict(X_train)\n",
        "\n",
        "# RMSE\n",
        "mean_squared_error(Y_train, Y_pred_train)**(1/2)"
      ],
      "metadata": {
        "id": "X3mMJ8OxUdrg"
      },
      "id": "X3mMJ8OxUdrg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on test set\n",
        "Y_pred_test = model2.predict(X_test)\n",
        "\n",
        "# RMSE\n",
        "mean_squared_error(Y_test, Y_pred_test)**(1/2)"
      ],
      "metadata": {
        "id": "mN5n1IhNUhpP"
      },
      "id": "mN5n1IhNUhpP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1eee1905",
      "metadata": {
        "id": "1eee1905"
      },
      "source": [
        "### 2c. Model Results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9107c597",
      "metadata": {
        "id": "9107c597"
      },
      "source": [
        "## 3. Final Model (Random Forest)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d53bffc",
      "metadata": {
        "id": "1d53bffc"
      },
      "source": [
        "### 3a. Prep/Split/Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_test split\n",
        "X = df_encoded[['Year', 'Assessed Personal Property Value', 'Year Property Built',\n",
        "       'Number of Bathrooms', 'Number of Bedrooms', 'Number of Rooms',\n",
        "       'Number of Stories', 'Number of Units', \n",
        "       'Incident Counts', 'safe_day', 'safe_night', 'household_size',\n",
        "       'cleanliness', 'Zipcode_94103', 'Zipcode_94104', 'Zipcode_94105',\n",
        "       'Zipcode_94107', 'Zipcode_94108', 'Zipcode_94109', 'Zipcode_94110',\n",
        "       'Zipcode_94111', 'Zipcode_94112', 'Zipcode_94114', 'Zipcode_94115',\n",
        "       'Zipcode_94116', 'Zipcode_94117', 'Zipcode_94118', 'Zipcode_94121',\n",
        "       'Zipcode_94122', 'Zipcode_94123', 'Zipcode_94124', 'Zipcode_94127',\n",
        "       'Zipcode_94131', 'Zipcode_94132', 'Zipcode_94133', 'Zipcode_94134',\n",
        "       'Zipcode_94158']]\n",
        "Y = df_encoded['Log of Total Assessed Value']\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = .3, random_state = 42)"
      ],
      "metadata": {
        "id": "rnm7I7IWUts8"
      },
      "id": "rnm7I7IWUts8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "30ed2f4f",
      "metadata": {
        "id": "30ed2f4f"
      },
      "source": [
        "### 3b. Model Definition/Run Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Model\n",
        "rf = RandomForestRegressor(n_estimators=10, random_state=42)  \n",
        "rf.fit(X_train, Y_train)"
      ],
      "metadata": {
        "id": "9EkijAiBV1He"
      },
      "id": "9EkijAiBV1He",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on test\n",
        "Y_pred2 = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(Y_test, Y_pred2)\n",
        "rmse = mse ** (1/2)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"Root Mean Squared Error: {rmse:.2f}\")"
      ],
      "metadata": {
        "id": "6N4bxKTSV6JO"
      },
      "id": "6N4bxKTSV6JO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Log Features & Response***"
      ],
      "metadata": {
        "id": "M1LuwF9HV-EV"
      },
      "id": "M1LuwF9HV-EV"
    },
    {
      "cell_type": "code",
      "source": [
        "#safe_day, safe_night, cleanliness --> convert these to binary (<3 not safe/clean, >= 3 safe/clean)\n",
        "df_encoded['safe_day_binary'] = np.where(df_encoded['safe_day']<3, 0, 1)\n",
        "df_encoded['safe_night_binary'] = np.where(df_encoded['safe_night']<3, 0, 1)\n",
        "df_encoded['cleanliness_binary'] = np.where(df_encoded['cleanliness']<3, 0, 1)"
      ],
      "metadata": {
        "id": "1Pn0QtBaWN67"
      },
      "id": "1Pn0QtBaWN67",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_encoded[['Year', 'Assessed Personal Property Value', 'Year Property Built',\n",
        "       'Number of Bathrooms', 'Number of Bedrooms', 'Number of Rooms',\n",
        "       'Number of Stories', 'Number of Units', \n",
        "       'Incident Counts', 'safe_day_binary', 'safe_night_binary', 'household_size',\n",
        "       'cleanliness_binary', 'Zipcode_94103', 'Zipcode_94104', 'Zipcode_94105',\n",
        "       'Zipcode_94107', 'Zipcode_94108', 'Zipcode_94109', 'Zipcode_94110',\n",
        "       'Zipcode_94111', 'Zipcode_94112', 'Zipcode_94114', 'Zipcode_94115',\n",
        "       'Zipcode_94116', 'Zipcode_94117', 'Zipcode_94118', 'Zipcode_94121',\n",
        "       'Zipcode_94122', 'Zipcode_94123', 'Zipcode_94124', 'Zipcode_94127',\n",
        "       'Zipcode_94131', 'Zipcode_94132', 'Zipcode_94133', 'Zipcode_94134',\n",
        "       'Zipcode_94158']]\n",
        "Y = df_encoded['Log of Total Assessed Value']\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = .3, random_state = 42)"
      ],
      "metadata": {
        "id": "O5Pyz_1DWTIk"
      },
      "id": "O5Pyz_1DWTIk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-define Model \n",
        "rf = RandomForestRegressor(n_estimators=10, random_state=42)  \n",
        "rf.fit(X_train, Y_train)"
      ],
      "metadata": {
        "id": "ZLo6x4RCWW8L"
      },
      "id": "ZLo6x4RCWW8L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on test\n",
        "Y_pred2 = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(Y_test, Y_pred2)\n",
        "rmse = mse ** (1/2)\n",
        "\n",
        "rmse_unlogged = mean_squared_error(np.exp(Y_test), np.exp(Y_pred2)) ** (1/2)\n",
        "\n",
        "\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"Root Mean Squared Error: {rmse:.2f}\")\n",
        "print(f\"Root Mean Squared Error (un-logged): {rmse_unlogged:.2f}\")"
      ],
      "metadata": {
        "id": "9YGvWz1QWa_l"
      },
      "id": "9YGvWz1QWa_l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RMSE\n",
        "rmse_unlogged = mean_squared_error(np.exp(Y_test), np.exp(Y_pred2)) ** (1/2)\n",
        "Y_pred2_train = rf.predict(X_train)\n",
        "rmse_unlogged_train = mean_squared_error(np.exp(Y_train), np.exp(Y_pred2_train)) ** (1/2)\n",
        "rmse_unlogged, rmse_unlogged_train"
      ],
      "metadata": {
        "id": "TJkSzUB-Wt8o"
      },
      "id": "TJkSzUB-Wt8o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's visualize this\n",
        "plt.scatter(np.arange(len(X_test)),np.exp(Y_pred2))\n",
        "plt.scatter(np.arange(len(X_test)),np.exp(Y_test))"
      ],
      "metadata": {
        "id": "D0zb_01bW3Mk"
      },
      "id": "D0zb_01bW3Mk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Increase n_estimators to 20\n",
        "rf2 = RandomForestRegressor(n_estimators=20, random_state=42)  \n",
        "rf2.fit(X_train, Y_train)"
      ],
      "metadata": {
        "id": "XKPtc5EzW8CI"
      },
      "id": "XKPtc5EzW8CI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on test\n",
        "Y_pred2 = rf2.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(Y_test, Y_pred2)\n",
        "rmse = mse ** (1/2)\n",
        "\n",
        "rmse_unlogged = mean_squared_error(np.exp(Y_test), np.exp(Y_pred2)) ** (1/2)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"Root Mean Squared Error: {rmse:.2f}\")\n",
        "print(f\"Root Mean Squared Error (un-logged): {rmse_unlogged:.2f}\")"
      ],
      "metadata": {
        "id": "oUuPZhyBW_Qt"
      },
      "id": "oUuPZhyBW_Qt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# On Train\n",
        "rmse_unlogged = mean_squared_error(np.exp(Y_test), np.exp(Y_pred2)) ** (1/2)\n",
        "Y_pred2_train = rf2.predict(X_train)\n",
        "rmse_unlogged_train = mean_squared_error(np.exp(Y_train), np.exp(Y_pred2_train)) ** (1/2)\n",
        "rmse_unlogged, rmse_unlogged_train"
      ],
      "metadata": {
        "id": "VOR6KoMoXI-b"
      },
      "id": "VOR6KoMoXI-b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's take another look\n",
        "plt.scatter(np.arange(len(X_test)),np.exp(Y_pred2))\n",
        "plt.scatter(np.arange(len(X_test)),np.exp(Y_test))"
      ],
      "metadata": {
        "id": "_-WX9WDNXSZa"
      },
      "id": "_-WX9WDNXSZa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Remove data points with valuation above 500 million to avoid outliers***"
      ],
      "metadata": {
        "id": "sODSxdpPXeRR"
      },
      "id": "sODSxdpPXeRR"
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's take a look\n",
        "plt.scatter(np.arange(len(X_train)),np.exp(Y_train))"
      ],
      "metadata": {
        "id": "XnFYmasNXj3p"
      },
      "id": "XnFYmasNXj3p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter for 'Total Assessed Value' < 250000000\n",
        "df_new = df_encoded[df_encoded['Total Assessed Value'] < 250000000]\n",
        "\n",
        "# train-test-split\n",
        "X = df_new[['Year', 'Assessed Personal Property Value', 'Year Property Built',\n",
        "       'Number of Bathrooms', 'Number of Bedrooms', 'Number of Rooms',\n",
        "       'Number of Stories', 'Number of Units', \n",
        "       'Incident Counts', 'safe_day_binary', 'safe_night_binary', 'household_size',\n",
        "       'cleanliness_binary', 'Zipcode_94103', 'Zipcode_94104', 'Zipcode_94105',\n",
        "       'Zipcode_94107', 'Zipcode_94108', 'Zipcode_94109', 'Zipcode_94110',\n",
        "       'Zipcode_94111', 'Zipcode_94112', 'Zipcode_94114', 'Zipcode_94115',\n",
        "       'Zipcode_94116', 'Zipcode_94117', 'Zipcode_94118', 'Zipcode_94121',\n",
        "       'Zipcode_94122', 'Zipcode_94123', 'Zipcode_94124', 'Zipcode_94127',\n",
        "       'Zipcode_94131', 'Zipcode_94132', 'Zipcode_94133', 'Zipcode_94134',\n",
        "       'Zipcode_94158']]\n",
        "Y = df_new['Log of Total Assessed Value']\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = .3, random_state = 42)"
      ],
      "metadata": {
        "id": "Ex7iofbTXubU"
      },
      "id": "Ex7iofbTXubU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-define Model\n",
        "rf3 = RandomForestRegressor(n_estimators=20, random_state=42)  \n",
        "rf3.fit(X_train, Y_train)"
      ],
      "metadata": {
        "id": "aOW24zdCXzpv"
      },
      "id": "aOW24zdCXzpv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on test\n",
        "Y_pred2 = rf3.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(Y_test, Y_pred2)\n",
        "rmse = mse ** (1/2)\n",
        "\n",
        "rmse_unlogged = mean_squared_error(np.exp(Y_test), np.exp(Y_pred2)) ** (1/2)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"Root Mean Squared Error: {rmse:.2f}\")\n",
        "print(f\"Root Mean Squared Error (un-logged): {rmse_unlogged:.2f}\")"
      ],
      "metadata": {
        "id": "npLiCMR3X5fy"
      },
      "id": "npLiCMR3X5fy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# On train\n",
        "rmse_unlogged = mean_squared_error(np.exp(Y_test), np.exp(Y_pred2)) ** (1/2)\n",
        "Y_pred2_train = rf3.predict(X_train)\n",
        "rmse_unlogged_train = mean_squared_error(np.exp(Y_train), np.exp(Y_pred2_train)) ** (1/2)\n",
        "rmse_unlogged, rmse_unlogged_train"
      ],
      "metadata": {
        "id": "b8UmRh9VX_FS"
      },
      "id": "b8UmRh9VX_FS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's take a look again\n",
        "plt.scatter(np.arange(len(X_test)),np.exp(Y_pred2))\n",
        "plt.scatter(np.arange(len(X_test)),np.exp(Y_test))\n",
        "# Looks to be the best performing model"
      ],
      "metadata": {
        "id": "xYO31hiyYEsP"
      },
      "id": "xYO31hiyYEsP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3d76f026",
      "metadata": {
        "id": "3d76f026"
      },
      "source": [
        "### 3c. Model Results"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}